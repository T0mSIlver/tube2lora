# Smoke config derived from configs/video_essay.yaml
# Uses legacy endpoint/model values, single test video, and small Whisper.

input:
  source_type: video
  source: "https://www.youtube.com/watch?v=9eRoYGhiDms"

download:
  include_shorts: false
  include_live: false
  media: audio
  keep_video: false
  yt_dlp_audio_format: "bestaudio/best"
  yt_dlp_video_format: "bestvideo+bestaudio/best"

transcribe:
  backend: faster_whisper
  prefer_manual_transcript: true
  allow_auto_generated: false
  language_allowlist: ["en"]
  faster_whisper:
    model_size: "small"
    device: cuda
    compute_type: auto
    beam_size: 5
    vad_filter: true

llm:
  base_url: "http://192.168.1.17:8000/v1"
  api_key_env: "OPENAI_API_KEY"
  timeout_seconds: 120
  max_retries: 3

normalize:
  enabled: false
  prompt_template: "../prompts/normalize_default.yaml"
  model: "mistralai/Ministral-3-14B-Instruct-2512"
  temperature: 0.1
  max_tokens: 4096
  chunk_target_chars: 3500
  chunk_overlap_chars: 0
  chunk_hard_max_chars: 5500

analyze:
  enabled: false
  minhash_num_perm: 128

filter:
  enabled: false
  min_words: 100
  min_tokens: 120
  language_allowlist: ["en"]
  dedup_threshold: 0.85
  min_quality_score: 0.2
  normalize_min_token_ratio: 0.7
  normalize_max_token_ratio: 1.35
  normalize_min_length_ratio: 0.7
  normalize_max_length_ratio: 1.35
  normalize_min_similarity: 0.65

generate:
  enabled: false
  prompt_template: "../prompts/video_essay_example.yaml"
  model: "mistralai/Ministral-3-14B-Instruct-2512"
  temperature: 0.1
  max_tokens: 4096
  buffer_chars: 7000
  fallback_min_chars: 2500
  fallback_max_chars: 3500
  min_tail_chars: 800

train:
  enabled: false
  base_model: "unsloth/Ministral-3-3B-Instruct-2512"
  load_in_4bit: true
  max_seq_length: 16384
  output_name: "smoke_small_adapter"
  lora_r: 64
  lora_alpha: 64
  lora_dropout: 0.05
  use_rslora: true
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 4
  warmup_steps: 15
  num_train_epochs: 1
  learning_rate: 1.0e-4
  weight_decay: 0.01
  lr_scheduler_type: "cosine"
  logging_steps: 5
  save_steps: 50
  dataset_num_proc: 2
  packing: false
  seed: 3407
  eval_split_ratio: 0.1

evaluate:
  enabled: false
  num_generation_samples: 1
  max_new_tokens: 128
  temperature: 0.7

pipeline:
  continue_on_error: true
  max_failure_rate: 0.2

paths:
  runs_dir: "../../runs"
