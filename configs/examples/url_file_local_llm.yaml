input:
  source_type: url_file
  source: "./urls.txt"

download:
  include_shorts: false
  include_live: false
  media: audio
  keep_video: false
  yt_dlp_audio_format: "bestaudio/best"
  yt_dlp_video_format: "bestvideo+bestaudio/best"

transcribe:
  backend: faster_whisper
  prefer_manual_transcript: true
  allow_auto_generated: false
  language_allowlist: ["en"]
  faster_whisper:
    model_size: "large-v3"
    device: auto
    compute_type: auto
    beam_size: 5
    vad_filter: true

llm:
  base_url: "http://127.0.0.1:8000/v1"
  api_key_env: "OPENAI_API_KEY"
  timeout_seconds: 120
  max_retries: 3

normalize:
  enabled: true
  prompt_template: "../prompts/normalize_default.yaml"
  model: "mistralai/Ministral-3-14B-Instruct-2512"
  temperature: 0.1
  max_tokens: 4096
  chunk_target_chars: 3500
  chunk_overlap_chars: 0
  chunk_hard_max_chars: 5500

analyze:
  enabled: true
  minhash_num_perm: 128

filter:
  enabled: true
  min_words: 100
  min_tokens: 120
  language_allowlist: ["en"]
  dedup_threshold: 0.85
  min_quality_score: 0.2
  normalize_min_token_ratio: 0.7
  normalize_max_token_ratio: 1.35
  normalize_min_length_ratio: 0.7
  normalize_max_length_ratio: 1.35
  normalize_min_similarity: 0.65

generate:
  enabled: true
  generators: ["talk_as_creator"]
  model: "mistralai/Ministral-3-14B-Instruct-2512"
  temperature: 0.1
  max_tokens: 4096
  talk_as_creator:
    prompt_template: "../prompts/talk_as_creator_default.yaml"
    turn_pairs: 3
    clarifying_question_probability: 0.4
    target_tokens_per_assistant_turn: 140
    min_source_tokens: 120
    max_copy_similarity: 0.8
    min_novel_token_ratio: 0.18
  editorial_rewrite:
    prompt_template: "../prompts/editorial_rewrite_default.yaml"
    briefs:
      - "cleanup_filler"
      - "tighten_60_percent"
      - "expand_with_examples"
      - "convert_to_script"
    use_normalized_input: true
    examples_per_chunk: 1
    min_source_tokens: 120
    cleanup_min_similarity: 0.45
    cleanup_max_similarity: 0.995
    tighten_target_ratio: 0.6
    tighten_ratio_tolerance: 0.25
    expand_min_ratio: 1.05
    expand_max_ratio: 2.6
    max_new_named_entities: 10

train:
  enabled: true
  base_model: "unsloth/Ministral-3-3B-Instruct-2512"
  load_in_4bit: true
  max_seq_length: 16384
  output_name: "ministral3b_tube2lora_adapter"
  lora_r: 64
  lora_alpha: 64
  lora_dropout: 0.05
  use_rslora: true
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 4
  warmup_steps: 15
  num_train_epochs: 4
  learning_rate: 1.0e-4
  weight_decay: 0.01
  lr_scheduler_type: "cosine"
  logging_steps: 5
  save_steps: 50
  dataset_num_proc: 2
  packing: false
  seed: 3407
  eval_split_ratio: 0.1

evaluate:
  enabled: true
  num_generation_samples: 3
  max_new_tokens: 200
  temperature: 0.7

pipeline:
  continue_on_error: true
  max_failure_rate: 0.2

paths:
  runs_dir: "../../runs"
